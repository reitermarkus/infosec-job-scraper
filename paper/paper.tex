\documentclass[runningheads]{llncs}


\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}

\begin{document}

\title{Analysis of Information Security Job Advertisements}

\author{Markus Reiter}

\institute{University of Innsbruck, Austria}

\maketitle

\newpage

\begin{abstract}
The abstract should briefly summarize the contents of the paper in
15--250 words.

\keywords{Information Security \and Scraping \and Job Advertisement}

\end{abstract}

\newpage

\section{Introduction}
\label{sec:introduction}


\section{Implementation}

\subsection{Evaluation of Scraping Tools}
\label{sub:evaluation_of_scraping_tools}

The first step towards being able to analyse job advertisements on the internet is being able to properly retrieve them. For this reason, we evaluated various online scraping services.

The first contender we found was \textit{OctoParse}, which provides a desktop application for scraping. Its free tier allows scraping unlimited pages on unlimited computers, with two concurrent scraping jobs. It allows configuring 10 different crawler configurations. There was, however, one major drawback to using \textit{OctoParse}: Its slogan claims “Easy Web Scraping for Anyone” – “Anyone” in this context meaning anyone with a Windows computer, since the desktop application is only supported on Windows. Linux and macOS users would have to run a virtual machine in order to use it, which is definitely not a viable solution for scraping a few websites. So we began looking for alternatives.

One alternative we found was \textit{ScrapingBot}. Unlike \textit{OctoParse}, which provides a desktop application, \textit{ScrapingBot} does not provide an application at all. It is meant to be used similarly to using a proxy server. You can make an HTTP request with the same URL you normally would, and it responds with the contents, which includes any rendered JavaScript. Its free plan includes 100 API calls per month, therefore this also was no viable solution either, since the API calls would probably have been used up before the scraper was even working properly.

The next service we found was \textit{ScrapeStorm}, which is very similar to \textit{OctoParse} in terms of what it provides. It offers a desktop application which works on Linux, macOS as well as Windows. In that regard, it was already looking like a better solution \textit{OctoParse}. The free plan allows 10 different crawler configurations and 1 concurrent local run. It also allows 100 exported rows per day, either into a file or directly into a database. All in all, \textit{ScrapeStorm} seemed like a good fit.

After choosing \textit{ScrapeStorm}, we started by setting up the crawlers. The websites we wanted to scrape are \texttt{monster.at}, \texttt{at.indeed.com} and \texttt{stepstone.at}.

For each website, we would have to implement two steps. First, we need to scrape the list of search results, which provides us with a list of links to the detail pages for the job advertisements. The second step is then to scrape the detail page itself.

As a first step, we tried scraping a detail page for a single job advertisement on \texttt{monster.at}. And it was a good thing we did, since it immediately highlighted a major problem. A job detail page on \texttt{monster.at} always includes the actual content of the advertisement inside of a nested \texttt{iframe} element. Using \textit{ScrapeStorm}, we were able to select that \texttt{iframe} element, but we failed to extract any data or text from it. The only thing we got was an empty string. This meant that we had to abandon the idea of using \textit{ScrapeStorm} for our purposes.

With this setback, we now had to find another way to scrape this data. After taking a second look at the \textit{ScrapingBot} homepage, we saw that it is using headless \textit{Chrome} for its backend. We decided to venture further into this direction and found \textit{Selenium}, which can be used to programmatically use a web browser.

\textit{Selenium} supports all of the major web browsers, the only thing you need is the so called “webdriver” for the corresponding browser. Additionally, \textit{Selenium} can be used with a wide array of programming languages. For our purposes we chose to go with the Ruby version of \textit{Selenium}. Specifically, we chose to use \textit{Rake}, which is similar to \textit{Make}, since it allows us to use the full functionality of \textit{Ruby} and specify tasks for each website. These scraping tasks can then be run with a simple \texttt{rake} command.

Given the setback we encountered on \texttt{monster.at}, the first thing we tried with \textit{Selenium} was to implement a simple task to extract the contents of the nested \texttt{iframe} element in their detail page layout. The first thing we needed to do was to fetch a job page. Next up, we needed to locate the nested \texttt{iframe} element; luckily this element is marked by a unique \texttt{id} field called \texttt{JobPreviewSandbox}. Once we had saved the nested \texttt{iframe} into a variable, we could use the \texttt{switch\_to} function provided by \textit{Selenium} to change to it from the main frame.

\begin{lstlisting}[language=Ruby]
iframe = find_element(id: 'JobPreviewSandbox')
switch_to.frame(iframe)
\end{lstlisting}

Seeing this proof-of-concept task working, we finally settled on using \textit{Selenium} and \textit{Rake} together with the \textit{Firefox} web browser.

\subsection{Scraping}
\label{sub:scraping}

As already briefly mentioned in \autoref{sub:evaluation_of_scraping_tools}, our goal was to implement scraping for \textit{Monster} (\href{https://www.monster.at/}{\texttt{monster.at}}), \textit{Indeed} (\href{https://at.indeed.com/}{\texttt{at.indeed.com}}) and \textit{StepStone} (\href{https://www.stepstone.at/}{\texttt{stepstone.at}}). In this section we will go into more detail about the exact steps which were necessary to reliably scrape each respective website.

\subsubsection{Monster}
\label{subsub:monster}

The first thing we needed to determine for \textit{Monster} was how to perform searches. For this, we simply made a search request manually in a web browser. With the search string “information security” immediately noticed that it was escaped as \texttt{information-security} rather than the standard way to escape HTML, i.e. \texttt{information\%20security}. We needed to consider this when building the query URL.

Next up, we needed to assess how search results are paginated. \textit{Monster} does not use a classic pagination method using query string parameters, but rather uses a single “infinite” list which loads new search results automatically when you scroll to the bottom, or alternatively, click the “load more jobs” button.

With \textit{Selenium}, it was quite easy to extract all URLs leading to detail pages from the result list. Loading more than the first “page” of results was, however, quite convoluted. Simply scrolling down was not a good option, since then there would be no good way of knowing when to stop scrolling. Conveniently, \textit{Selenium} provides function to scroll to a given element, so we simply tried selecting the “load more jobs” button and scrolling to it. This is where we hit another problem. We could scroll the the button, but we were not able to click on it due to a pop-up blocking exactly the lower part of the page where the button is located, which meant we first had to dismiss this pop-up, which further meant introducing another potential point of failure, but for the moment, this solution was working.

In essence, the workflow for parsing URLs was as follows:

\begin{enumerate}
  \item Perform search request.
  \item Locate pop-up and dismiss it.
  \item Locate “load more jobs” button.
  \item \label{monster_workflow_1_scroll_to_button}
    Scroll to “load more jobs” button if it exists, otherwise go to step \ref{monster_workflow_1_select_results}.
  \item \label{monster_workflow_1_click_button}
    Click “load more jobs” button.
  \item \label{monster_workflow_1_select_results}
    Select all result URLs and collect them into an array.
\end{enumerate}

This workflow seemed to work as expected, however, we later encountered spurious error between step \ref{monster_workflow_1_scroll_to_button} and \ref{monster_workflow_1_click_button}. As mentioned previously, new search results are automatically loaded when you scroll to the bottom of the page. Obviously, scrolling to the “load more jobs” button was triggering that functionality and render the reference to the button invalid, making step \ref{monster_workflow_1_click_button} fail. While searching for a solution, we came upon \textit{Selenium's} \texttt{execute\_script} function, which can be used to directly execute \textit{JavaScript} code in the current browser window. This function enabled us to directly click the “load more jobs” button without scrolling to it, which also made it possible to eliminate the step for locating and dismissing the pop-up.

The final workflow now looked like this:

\begin{enumerate}
  \item Perform search request.
  \item Locate “load more jobs” button.
  \item Click the “load more jobs” button if it exists, otherwise go to \ref{monster_workflow_2_select_results}.
  \item \label{monster_workflow_2_select_results}
    Select all result URLs and collect them into an array.
\end{enumerate}

\subsubsection{Indeed}
\label{subsub:indeed}

For \textit{Indeed}, the initial step to implement proper scraping was to implement fetching search results. Luckily, unlike on \textit{Monster}, the search string could be escaped as \texttt{information\%20security} using standardised HTML escape codes.

Pagination on \textit{Indeed} is using a query parameter called \texttt{start} which indicates the offset from the first result. Additionally, the pagination buttons are located at the bottom of the results page. With the “next page” button acting as the stop indicater we can trivially implement the gathering of all detail page URLs as described below:

\begin{enumerate}
  \item Initialize empty array.
  \item \label{indeed_workflow_1_search}
    Perform search request with offset \texttt{0}.
  \item Select all result URLs and add them to the array.
  \item If “next page” button exists, repeat step \ref{indeed_workflow_1_search} with offset + \texttt{10}.
\end{enumerate}

\subsubsection{StepStone}
\label{subsub:stepstone}

Implementing the gathering of search results on \textit{StepStone} was largely similar to the workflow used for \textit{Indeed}. The search string can be escaped using the standardised HTML escape codes, furthermore there are some query parameters we make use of: The \texttt{fu} parameter can be used to set the job category. We used \texttt{1000000} for this parameter to specify the “IT \& Communications” category. For faster scraping, we also set the \texttt{li} parameter to \texttt{100} in order to display 100 results per page rather than the default 25. Finally, we use the \texttt{of} parameter to specify the result offset which is then used in the same way as for \texttt{Indeed}. \textit{StepStone} displays the pagination controls at the top of the results page. These include the current as well as the last page number. Using this information we can stop the loop once we reach the last page.

\subsection{Natural Language Processing}
\label{sub:natural_language_processing}


\subsection{Graph Generation}
\label{sub:graph_generation}

\end{document}
