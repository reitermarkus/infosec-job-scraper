\documentclass[runningheads]{llncs}

\usepackage{fontspec}

\usepackage{multicol}

\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}

\usepackage[
  style   = numeric,
  sorting = none,
]{biblatex}
\bibliography{paper}

\begin{document}

\title{Analysis of Information Security Job Advertisements}

\author{Markus Reiter}

\institute{University of Innsbruck, Austria}

\maketitle

\newpage

\begin{abstract}
The abstract should briefly summarise the contents of the paper in
15--250 words.

\keywords{Information Security \and Scraping \and Job Advertisement}

\end{abstract}

\newpage

\section{Introduction}
\label{sec:introduction}

Information security management is more and more becoming an essential area of expertise for large and small companies alike.

\section{Evaluation of Scraping Tools}
\label{sec:evaluation_of_scraping_tools}

The first step towards being able to analyse job advertisements on the internet is being able to properly retrieve them. For this reason, we evaluated various online scraping services.

The first contender we found was \textit{OctoParse}, which provides a desktop application for scraping. Its free tier allows scraping unlimited pages on unlimited computers, with two concurrent scraping jobs. It allows configuring 10 different crawler configurations. There was, however, one major drawback to using \textit{OctoParse}: Its slogan claims “Easy Web Scraping for Anyone” – “Anyone” in this context meaning anyone with a Windows computer, since the desktop application is only supported on Windows. Linux and macOS users would have to run a virtual machine in order to use it, which is definitely not a viable solution for scraping a few websites. So we began looking for alternatives.

One alternative we found was \textit{ScrapingBot}. Unlike \textit{OctoParse}, which provides a desktop application, \textit{ScrapingBot} does not provide an application at all. It is meant to be used similarly to using a proxy server. You can make an HTTP request with the same URL you normally would, and it responds with the contents, which includes any rendered JavaScript. Its free plan includes 100 API calls per month, therefore this also was no viable solution either, since the API calls would probably have been used up before the scraper was even working properly.

The next service we found was \textit{ScrapeStorm}, which is very similar to \textit{OctoParse} in terms of what it provides. It offers a desktop application which works on Linux, macOS as well as Windows. In that regard, it was already looking like a better solution \textit{OctoParse}. The free plan allows 10 different crawler configurations and 1 concurrent local run. It also allows 100 exported rows per day, either into a file or directly into a database. All in all, \textit{ScrapeStorm} seemed like a good fit.

After choosing \textit{ScrapeStorm}, we started by setting up the crawlers. The websites we wanted to scrape are \texttt{monster.at}, \texttt{at.indeed.com} and \texttt{stepstone.at}.

For each website, we would have to implement two steps. First, we need to scrape the list of search results, which provides us with a list of links to the detail pages for the job advertisements. The second step is then to scrape the detail page itself.

As a first step, we tried scraping a detail page for a single job advertisement on \texttt{monster.at}. And it was a good thing we did, since it immediately highlighted a major problem. A job detail page on \texttt{monster.at} always includes the actual content of the advertisement inside of a nested \texttt{iframe} element. Using \textit{ScrapeStorm}, we were able to select that \texttt{iframe} element, but we failed to extract any data or text from it. The only thing we got was an empty string. This meant that we had to abandon the idea of using \textit{ScrapeStorm} for our purposes.

With this setback, we now had to find another way to scrape this data. After taking a second look at the \textit{ScrapingBot} homepage, we saw that it is using headless \textit{Chrome} for its backend. We decided to venture further into this direction and found \textit{Selenium}, which can be used to programmatically use a web browser.

\textit{Selenium} supports all of the major web browsers, the only thing you need is the so called “webdriver” for the corresponding browser. Additionally, \textit{Selenium} can be used with a wide array of programming languages. For our purposes we chose to go with the Ruby version of \textit{Selenium}. Specifically, we chose to use \textit{Rake}, which is similar to \textit{Make}, since it allows us to use the full functionality of \textit{Ruby} and specify tasks for each website. These scraping tasks can then be run with a simple \texttt{rake} command.

Given the setback we encountered on \texttt{monster.at}, the first thing we tried with \textit{Selenium} was to implement a simple task to extract the contents of the nested \texttt{iframe} element in their detail page layout. The first thing we needed to do was to fetch a job page. Next up, we needed to locate the nested \texttt{iframe} element; luckily this element is marked by a unique \texttt{id} field called \texttt{JobPreviewSandbox}. Once we had saved the nested \texttt{iframe} into a variable, we could use the \texttt{switch\_to} function provided by \textit{Selenium} to change to it from the main frame.

\begin{lstlisting}[language=Ruby]
iframe = find_element(id: 'JobPreviewSandbox')
switch_to.frame(iframe)
\end{lstlisting}

Seeing this proof-of-concept task working, we finally settled on using \textit{Selenium} and \textit{Rake} together with the \textit{Firefox} web browser.

\section{Implementation}

\subsection{Scraping}
\label{sub:scraping}

As already briefly mentioned in \autoref{sec:evaluation_of_scraping_tools}, our goal was to implement scraping for \textit{Monster} (\href{https://www.monster.at/}{\texttt{monster.at}}), \textit{Indeed} (\href{https://at.indeed.com/}{\texttt{at.indeed.com}}) and \textit{StepStone} (\href{https://www.stepstone.at/}{\texttt{stepstone.at}}). In this section we will go into more detail about the exact steps which were necessary to reliably scrape each respective website.

\subsubsection{Monster}
\label{subsub:monster}

The first thing we needed to determine for \textit{Monster} was how to perform searches. For this, we simply made a search request manually in a web browser. With the search string “information security” immediately noticed that it was escaped as \texttt{information-security} rather than the standard way to escape HTML, i.e. \texttt{information\%20security}. We needed to consider this when building the query URL.

Next up, we needed to assess how search results are paginated. \textit{Monster} does not use a classic pagination method using query string parameters, but rather uses a single “infinite” list which loads new search results automatically when you scroll to the bottom, or alternatively, click the “load more jobs” button.

With \textit{Selenium}, it was quite easy to extract all URLs leading to detail pages from the result list. Loading more than the first “page” of results was, however, quite convoluted. Simply scrolling down was not a good option, since then there would be no good way of knowing when to stop scrolling. Conveniently, \textit{Selenium} provides function to scroll to a given element, so we simply tried selecting the “load more jobs” button and scrolling to it. This is where we hit another problem. We could scroll the the button, but we were not able to click on it due to a pop-up blocking exactly the lower part of the page where the button is located, which meant we first had to dismiss this pop-up, which further meant introducing another potential point of failure, but for the moment, this solution was working.

In essence, the workflow for parsing URLs was as follows:

\begin{enumerate}
  \item Perform search request.
  \item Locate pop-up and dismiss it.
  \item Locate “load more jobs” button.
  \item \label{monster_workflow_1_scroll_to_button}
    Scroll to “load more jobs” button if it exists, otherwise go to step \ref{monster_workflow_1_select_results}.
  \item \label{monster_workflow_1_click_button}
    Click “load more jobs” button.
  \item \label{monster_workflow_1_select_results}
    Select all result URLs and collect them into an array.
\end{enumerate}

This workflow seemed to work as expected, however, we later encountered spurious error between step \ref{monster_workflow_1_scroll_to_button} and \ref{monster_workflow_1_click_button}. As mentioned previously, new search results are automatically loaded when you scroll to the bottom of the page. Obviously, scrolling to the “load more jobs” button was triggering that functionality and render the reference to the button invalid, making step \ref{monster_workflow_1_click_button} fail. While searching for a solution, we came upon \textit{Selenium's} \texttt{execute\_script} function, which can be used to directly execute \textit{JavaScript} code in the current browser window. This function enabled us to directly click the “load more jobs” button without scrolling to it, which also made it possible to eliminate the step for locating and dismissing the pop-up.

The final workflow now looked like this:

\begin{enumerate}
  \item Perform search request.
  \item Locate “load more jobs” button.
  \item Click the “load more jobs” button if it exists, otherwise go to \ref{monster_workflow_2_select_results}.
  \item \label{monster_workflow_2_select_results}
    Select all result URLs and collect them into an array.
\end{enumerate}

For the second part, the detail page of the job advertisement, we could now loop through all URLs and extract information from each one. On \textit{Monster}, each detail page contains the job title and a subtitle. The subtitle usually contains the location of the open position. Additionally, there is a sidebar which contains further metadata like the contract type, category, carreer level and publication date. The main content of the job advertisement is embedded into a nested \texttt{iframe} element. As mentioned in \autoref{sec:evaluation_of_scraping_tools}, using \textit{Selenium}, we could easily extract the content of this \texttt{iframe} element using the \texttt{switch\_to} function. All other data we could extract equally easily by selecting the corresponding elements using CSS selectors.

\subsubsection{Indeed}
\label{subsub:indeed}

For \textit{Indeed}, the initial step to implement proper scraping was to implement fetching search results. Luckily, unlike on \textit{Monster}, the search string could be escaped as \texttt{information\%20security} using standardised HTML escape codes.

Pagination on \textit{Indeed} is using a query parameter called \texttt{start} which indicates the offset from the first result. Additionally, the pagination buttons are located at the bottom of the results page. With the “next page” button acting as the stop indicater we can trivially implement the gathering of all detail page URLs as described below:

\begin{enumerate}
  \item Initialise empty array.
  \item \label{indeed_workflow_1_search}
    Perform search request with offset \texttt{0}.
  \item Select all result URLs and add them to the array.
  \item If “next page” button exists, repeat step \ref{indeed_workflow_1_search} with offset + \texttt{10}.
\end{enumerate}

Similarly to \textit{Monster}, the detail pages on \textit{Indeed} contain the title of the job advertisement as well as a metadata section. Care has to be taken when extracting data from this metadata section, since not every detail page contains every entry. Most pages seem to contain the location, but only some contain the contract type of the open position. For this reason, we simply treat all of these as optional and ignore any \texttt{NoSuchElementError} when searching for the respective element. In contrast to \textit{Monster}, the main content of the advertisement is contained as plain text, so no special treatment is necessary in this case.

\subsubsection{StepStone}
\label{subsub:stepstone}

Implementing the gathering of search results on \textit{StepStone} was largely similar to the workflow used for \textit{Indeed}. The search string can be escaped using the standardised HTML escape codes, furthermore there are some query parameters we make use of: The \texttt{fu} parameter can be used to set the job category. We used \texttt{1000000} for this parameter to specify the “IT \& Communications” category. For faster scraping, we also set the \texttt{li} parameter to \texttt{100} in order to display 100 results per page rather than the default 25. Finally, we use the \texttt{of} parameter to specify the result offset which is then used in the same way as for \texttt{Indeed}. \textit{StepStone} displays the pagination controls at the top of the results page. These include the current as well as the last page number. Using this information we can stop the loop once we reach the last page.

A detail page on \textit{StepStone} contains the job title at the top, followed by a metadata section. The metadata contains the location, type of contract and type of employment (i.e. full-time or part-time). The main content of the job advertisement, similarly to \textit{Indeed}, is a section containing plain text. This makes it trivial to extract all data.

\subsubsection{Storage of Scraped Data}
\label{subsub:storage_of_scraped_data}

Now that we had successfully built the foundation for properly scraping all websites, the last step was to store the scraped data in a format which can be used for further processing. Using a full-fledged database solution, e.g. MySQL or Postgres, was out of the question from the start. Setting up a dedicated database server for what is -- relatively speaking -- a very small amount of data would add too much maintenance overhead. Another aspect to consider is the data layout.
In database terms, we need to store only a single table of data, which was another reason why we decided against using a database for storing the collected data altogether. This included embedded databases like SQLite which consist of a single file and don't need a dedicated server and client. Ultimately we decided on storing each scraping result in a separate \textit{JSON} file. \textit{JSON} is heavily used for APIs on the web which means that virtually every programming language supports deserialising and serialising data in \textit{JSON} format. To avoid duplicates, we chose to save the files as \texttt{<website>-<id>.json} where \texttt{<website>} is the name of the website, i.e. \texttt{monster}, \texttt{indeed} or \texttt{stepstone}. For the \texttt{id} we calculate the SHA2 checksum of the URL.

For example, the scraped data from the URL

\url{https://at.indeed.com/rc/clk?jk=db6f5459c93e648c&fccid=1272531993b16790&vjs=3}

would be saved as

\texttt{indeed-43da336235334055a25f32a55e7cd343128905d782cd749349659ca3d3d38174.json}

\subsubsection{Removal of Duplicates}
\label{subsub:removal_of_duplicates}

After a couple of scraping runs, we noticed that there were duplicate results. This happened for all three websites. These duplicates were either completely equal or had only minor differences in their HTML source code. Nevertheless, we needed to introduce a post-processing step after the scraping is finished. We implemented a new \textit{Rake} task called \texttt{deduplicate} which reads all results and sorts them by date. We then convert the results' title and HTML body content to plain text and group the results according to that. Finally, we select all results with more than one source file and remove all but the latest one.

\subsection{Natural Language Processing}
\label{sub:natural_language_processing}

Once the scraping was done, we could focus on extracting information from the scraped data. For the purpose of data extraction, we chose Python. Like Ruby, Python is an interpreted and dynamically typed language, which makes it a good canditate for natural language processing since we can iterate over code changes very quickly. We chose Python over Ruby due to the vast amount of libraries and frameworks available dedicated to natural language processing, first and foremost the Natural Language Toolkit (NLTK), arguably one of the most widely used platforms for natural language processing. It contains a great variety of tools including classification, tokenization, stemming, tagging, parsing and semantic reasoning. \cite{nltk}

\subsubsection{Data Cleaning}
\label{subsub:data_cleaning}

The first step when performing any sort of natural language processing should be data cleaning. Specifically, this means removing irrelevant data and bringing the data into a format that can be processed more easily than the raw source. In our the main part that needed to be cleaned was the body text of each result. Each result contains the raw HTML source code of the body text, so the first step was to strip the text from any contained HTML tags. We used the aptly named \texttt{html2text} Python package for this purpose. After further inspection of the results, we noticed that some contained formatting, links and images in Markdown format. We quickly realised that \texttt{html2text} is in fact an HTML to Markdown converter instead of a HTML to plain text converter as we previously deemed from the name. Luckily, \texttt{html2text} provides \texttt{ignore\_emphasis}, \texttt{ignore\_links} and \texttt{ignore\_images} options which can be used to remove the Markdown output for these elements. Once we had the plain text version of the body content, we could subsequently use the \texttt{word\_tokenize} function from the NLTK to split the text into tokens to make further processing easier.

\subsubsection{Extracting the Salary}
\label{subsub:extracting_the_salary}

The first data point we decided to extract was the salary for the advertised job. Even with only this single data point, there is a lot of variation in how the salary is represented.

\begin{figure}
\begin{multicols}{3}
\begin{description}
  \item 43,595 Euros
  \item 30k Euros
  \item 3,100.00 EUR
  \item EUR 2.800,-
  \item EUR 50k
  \item EUR 5700
  \item EUR 2.600,00
  \item 70.000 €
  \item € 4.209,-
  \item € 3.933,0
  \item € 55.000,--
  \item € 65.000,---
  \item 65,000 €
  \item €60,000
  \item €85k
  \item €3.007,20
  \item 80.000 Euro
  \item 60000 €
\end{description}
\end{multicols}
\caption{An incomplete list (extracted from actual data) of number formats used to represent salary.}
\label{salary_formats}
\end{figure}

As seen in \autoref{salary_formats}, the diversity of number formats is quite extensive. To explain the steps we took to further clean the input data, we will define the \texttt{clean\_text} step as cleaning the complete text before it is tokenised and the \texttt{clean\_word} step as cleaning a single word after tokenisation.

The first step we took was to add another substep to our existing \texttt{clean\_text} step in which we used the regular expression \texttt{/(EUR|€)/} and replace it with \texttt{' \textbackslash1 '}, i.e. we add an extra space around \texttt{EUR} and \texttt{€} to cover the case where there is not already a space between the currency and the amount. We don't care about introducing extra spacing when there already is a space since the tokenisation will remove it anyways.

Now that we have ensured that the currency is separated from the amound, we can add another substep to \texttt{clean\_word}. Here we introduce a trivial replacement and replace all words which are equal to \texttt{'€'}, \texttt{'eur'}, \texttt{'euro'} or \texttt{'euros'} with \texttt{'€'}.

Next, we add another substep to \texttt{clean\_word} which removes any trailing \texttt{',-'} from numbers. We use a regular expression here to also catch cases with more than a single hyphen. Similarly to the trailing \texttt{',-'}, we replace a trailing \texttt{'k'} with \texttt{'000'}.

The only thing left to do now is to normalise the decimal format (i.e. \texttt{XXXXX,XX}, \texttt{XXXXX.XX}, \texttt{XX.XXX,XX}) \texttt{XX,XXX.XX}, to \texttt{XXXXX.XX} in order to be able parse it in a later step. For the normalisation, we assume that the salary amount has at most two decimal places. We use a regular expression to match 0-2 decimal places with any delimiter symbol and group everything else together. This gives use the integer part with all delimiter symbols and the decimal part as only numbers. Now we simply remove all delimiters from the integer part and combine both parts again with a \texttt{'.'}.

In order to eventually extract the salary, we loop through all cleaned tokens as 2-grams so we can match the currency symbol either before or after the number.

\subsubsection{Extracting the Type of Education}
\label{subsub:extracting_the_type_of_education}

Another data point we wanted to extract is the level of education required for a given job. We started by looking at some advertisements and then began building some patterns which we could use in our \texttt{guess\_education} function. Some obvious patterns include \texttt{<type>-Abschluss} and \texttt{<type>abschluss} with concrete examples being “HAK-Abschluss” and “Pflichtschulabschluss”. Since the \texttt{guess\_education} function receives the input as individual tokens, compound words including acronyms such as “HTL-Abschluss” would already be split into \texttt{'htl'} and \texttt{'abschluss'}. Given that there are only a handful of acronyms we decided for simplicity's sake to hardcode the most common ones: \texttt{'hak'}, \texttt{'hbla'}, \texttt{'htl'}, \texttt{'hlw'} and \texttt{'lfs'}. We grouped all of these into the \texttt{matura} category. Still, there were some patterns which fall into this category which were not yet recognized, such as “Matura” as a standalone term or “Fachschulabschluss”. For these, we simply check if any word contains these terms.

The next two obvious education types were the bachelor's and master's degree. For these, we again simply checked if any word contains the respective term and categorise them accordingly. Another related education type is an FH (Fachhochschule) degree which can be equivalent to either a bachelor's or master's degree. Here we again took the same approach as for the \texttt{matura} category, since both “FH-Abschluss” and “Fachhochschulabschluss” are commonly used terms.

With these types for higher levels of education covered, we could guess the education level only for less than half of the results. One type we missed previously was the completion of compulsory school. With compulsory school added to the list, we still were only detecting an education type for around half of all results. After inspecting some of the results which were still missing an education type, we found the two types which were still missing: completed apprenticeship (often referred to as “abgeschlossene Ausbildung”) and completed apprenticeship (often referred to as “abgeschlossene Lehre”).

With these two types the extraction rate was now at approximately 90\%. A few more results could be detected by extracting the word \texttt{'studium'}, since some job advertisements refer to a bachelor's degree simply as “abgeschlossenes Studium” (completed studies).

\subsection{Graph Generation}
\label{sub:graph_generation}

\newpage
\printbibliography

\end{document}
